{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "!pip install google-genai pandas numpy python-dotenv json5",
   "id": "5be9400855b2d374",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 1. Imports und Basis-Konfiguration ---\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "try:\n",
    "    from google import genai\n",
    "except ImportError:\n",
    "    raise ImportError(\"Installiere google-genai: pip install google-genai\")\n",
    "\n",
    "# Warnungen unterdr√ºcken (f√ºr saubere Ausgabe)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Lade Umgebungsvariablen (API-Key)\n",
    "load_dotenv()  # L√§dt GEMINI_API_KEY aus .env Datei\n",
    "\n",
    "# --- 2. Zentral Konfiguration (skalierbar - nur hier anpassen!) ---\n",
    "CONFIG = {\n",
    "    # Aktienliste (erweiterbar um beliebige Ticker)\n",
    "    \"AKTIEN_TICKER\": [\"ASML\", \"META\", \"AMZN\", \"TSLA\", \"NVDA\"],  # NVIDIA = NVDA (Standard-Ticker)\n",
    "    # Dateipfade (skalierbar - nur Pfade anpassen)\n",
    "    \"NEWS_CSV_PFAD\": \"news_6m_finnhub_newsapi.csv\",\n",
    "    \"KURS_JSON_ORDNER\": \"Stockcorse_as_json/\",  # z.B. Stockcorse_as_json/AMZN_schlusskurse.json\n",
    "    # Spaltennamen (passend zu deiner Finnhub-News-CSV)\n",
    "    \"NEWS_SPALTEN\": {\n",
    "        \"ticker\": \"ticker\",\n",
    "        \"title\": \"title\",\n",
    "        \"url\": \"url\",\n",
    "        \"published_at\": \"published_at_utc\",\n",
    "        \"provider\": \"provider\",\n",
    "        \"channel\": \"channel\",\n",
    "        \"news_source\": \"news_source\",\n",
    "        \"keyword\": \"keyword\"\n",
    "    },\n",
    "    # Gemini Konfiguration\n",
    "    \"GEMINI_MODEL\": \"gemini-2.5-flash\",\n",
    "    \"TEMPERATUR\": 0.0,  # Keine Variabilit√§t (objektive Ergebnisse)\n",
    "    # Filter-Schwellen (anpassbar)\n",
    "    \"MIN_KURS_RELEVANZ\": 0.5,  # Mindestscore f√ºr Kursrelevanz (0-1)\n",
    "    \"MIN_TREND_RELEVANZ\": 0.5   # Mindestscore f√ºr Trendrelevanz (0-1)\n",
    "}\n",
    "\n",
    "# --- 3. Hilfsfunktionen (skalierbar) ---\n",
    "def init_gemini_client():\n",
    "    \"\"\"\n",
    "    Lazy-Initialisierung des Gemini-Clients (Singleton)\n",
    "    Skalierbar f√ºr verschiedene Modelle\n",
    "    \"\"\"\n",
    "    if getattr(init_gemini_client, \"client\", None) is None:\n",
    "        api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise EnvironmentError(\n",
    "                \"Gemini API-Key fehlt! Setze die Umgebungsvariable GEMINI_API_KEY oder speichere sie in einer .env Datei.\"\n",
    "            )\n",
    "        genai.configure(api_key=api_key)\n",
    "        init_gemini_client.client = genai.GenerativeModel(CONFIG[\"GEMINI_MODEL\"])\n",
    "    return init_gemini_client.client\n",
    "\n",
    "def lade_news_daten():\n",
    "    \"\"\"\n",
    "    L√§dt die News-Daten aus der CSV-Datei (skalierbar f√ºr beliebige Ticker)\n",
    "    Returns: DataFrame mit bereinigten News-Daten\n",
    "    \"\"\"\n",
    "    if not Path(CONFIG[\"NEWS_CSV_PFAD\"]).exists():\n",
    "        raise FileNotFoundError(f\"News-CSV nicht gefunden: {CONFIG['NEWS_CSV_PFAD']}\")\n",
    "\n",
    "    df = pd.read_csv(CONFIG[\"NEWS_CSV_PFAD\"])\n",
    "\n",
    "    # Pr√ºfe auf erforderliche Spalten\n",
    "    required_cols = list(CONFIG[\"NEWS_SPALTEN\"].values())\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise KeyError(f\"Fehlende Spalten in News-CSV: {missing_cols}\")\n",
    "\n",
    "    # Filter nur relevante Ticker (skalierbar)\n",
    "    df = df[df[CONFIG[\"NEWS_SPALTEN\"][\"ticker\"]].isin(CONFIG[\"AKTIEN_TICKER\"])].reset_index(drop=True)\n",
    "\n",
    "    # Datum formatieren (UTC -> datetime)\n",
    "    df[\"published_at_datetime\"] = pd.to_datetime(df[CONFIG[\"NEWS_SPALTEN\"][\"published_at\"]], errors=\"coerce\")\n",
    "\n",
    "    print(f\"‚úÖ {len(df)} News-Eintr√§ge geladen (nur f√ºr {CONFIG['AKTIEN_TICKER']})\")\n",
    "    return df\n",
    "\n",
    "def lade_kursdaten_fuer_ticker(ticker):\n",
    "    \"\"\"\n",
    "    L√§dt die JSON-Kursdaten f√ºr einen einzelnen Ticker (skalierbar)\n",
    "    Args:\n",
    "        ticker: Aktien-Ticker (z.B. \"AMZN\")\n",
    "    Returns: DataFrame mit Datum und Schlusskurs\n",
    "    \"\"\"\n",
    "    json_pfad = Path(CONFIG[\"KURS_JSON_ORDNER\"]) / f\"{ticker}_schlusskurse.json\"\n",
    "    if not json_pfad.exists():\n",
    "        raise FileNotFoundError(f\"Kurs-JSON nicht gefunden: {json_pfad}\")\n",
    "\n",
    "    # JSON laden und parsen\n",
    "    with open(json_pfad, \"r\", encoding=\"utf-8\") as f:\n",
    "        kurs_daten = json.load(f)\n",
    "\n",
    "    # JSON-Struktur aufbereiten: [{\"('Schlusskurs', 'AMZN')\": 206.16}, ...] -> DataFrame\n",
    "    schlusskurse = []\n",
    "    for entry in kurs_daten:\n",
    "        for key, value in entry.items():\n",
    "            # Extrahiere Wert (ignoriere Key-String, nur Wert relevant)\n",
    "            schlusskurse.append(value)\n",
    "\n",
    "    # Datum generieren (letzte 6 Monate, passend zur Anzahl der Kurseintr√§ge)\n",
    "    anzahl_tage = len(schlusskurse)\n",
    "    start_datum = datetime.now() - timedelta(days=anzahl_tage)\n",
    "    daten = pd.date_range(start=start_datum, periods=anzahl_tage, freq=\"D\")\n",
    "\n",
    "    df_kurs = pd.DataFrame({\n",
    "        \"Datum\": daten,\n",
    "        \"Schlusskurs\": schlusskurse,\n",
    "        \"Ticker\": ticker\n",
    "    })\n",
    "\n",
    "    return df_kurs\n",
    "\n",
    "def lade_alle_kursdaten():\n",
    "    \"\"\"\n",
    "    L√§dt Kursdaten f√ºr alle Ticker in CONFIG (skalierbar)\n",
    "    Returns: Zusammengef√ºhrtes DataFrame mit Kursdaten aller Aktien\n",
    "    \"\"\"\n",
    "    alle_kursdaten = []\n",
    "    for ticker in CONFIG[\"AKTIEN_TICKER\"]:\n",
    "        try:\n",
    "            df_ticker = lade_kursdaten_fuer_ticker(ticker)\n",
    "            alle_kursdaten.append(df_ticker)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"‚ö†Ô∏è {e} - √úberspringe diesen Ticker\")\n",
    "\n",
    "    df_kurs_gesamt = pd.concat(alle_kursdaten, ignore_index=True)\n",
    "    print(f\"‚úÖ Kursdaten f√ºr {len(alle_kursdaten)} Ticker geladen\")\n",
    "    return df_kurs_gesamt\n",
    "\n",
    "# --- 4. Filterfunktionen (Kurs- und Trendrelevanz) ---\n",
    "def bewerte_relevanz(news_text, ticker):\n",
    "    \"\"\"\n",
    "    Bewertet eine News nach:\n",
    "    1. Kursrelevanz (0-1): Potenzial zur Beeinflussung des Aktienkurses\n",
    "    2. Trendrelevanz (0-1): Potenzial zur Beeinflussung des Trends (up/down)\n",
    "    3. Trendrichtung (up/down/neutral)\n",
    "    Skalierbar: Anpassbare Prompt-Regeln\n",
    "    \"\"\"\n",
    "    client = init_gemini_client()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Du bist ein erfahrener Finanzanalyst. Bewerte die folgende News f√ºr den Aktien-Ticker {ticker}:\n",
    "\n",
    "    ANWEISUNGEN:\n",
    "    1. BEWERTE KURSRELEVANZ (0-1):\n",
    "       - 0: Kein Einfluss auf den Aktienkurs (z.B. unwichtige Nachrichten)\n",
    "       - 1: Starker Einfluss (z.B. Produktionsbruch, Umsatzmeldungen, gro√üe Investitionen)\n",
    "    2. BEWERTE TRENDELEVANZ (0-1):\n",
    "       - 0: Kein Einfluss auf den Trend (up/down)\n",
    "       - 1: Starker Einfluss auf den Trend (z.B. Produktionsbruch ‚Üí down-Trend, hohe Ums√§tze ‚Üí up-Trend)\n",
    "    3. GIB TRENDRICHTUNG AN:\n",
    "       - up: News beg√ºnstigt steigenden Kurs\n",
    "       - down: News beg√ºnstigt fallenden Kurs\n",
    "       - neutral: Keine klare Richtung\n",
    "\n",
    "    GIB DIE ANTWORT IN FOLGENDEM FORMAT ZUR√úCK (NUR JSON, KEINE WEITEREN TEXTEN!):\n",
    "    {{\"kurs_relevanz\": 0.8, \"trend_relevanz\": 0.7, \"trend_richtung\": \"down\"}}\n",
    "\n",
    "    NEWS-TEXT: {news_text[:3000]}  # Begrenze L√§nge f√ºr Performance\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate_content(prompt)\n",
    "        # JSON parsen\n",
    "        relevanz_ergebnis = json.loads(response.text.strip())\n",
    "        return (\n",
    "            relevanz_ergebnis[\"kurs_relevanz\"],\n",
    "            relevanz_ergebnis[\"trend_relevanz\"],\n",
    "            relevanz_ergebnis[\"trend_richtung\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fehler bei Relevanzbewertung: {e}\")\n",
    "        return 0.0, 0.0, \"neutral\"\n",
    "\n",
    "def filter_news_nach_relevanz(df_news):\n",
    "    \"\"\"\n",
    "    Filtert News nach Kurs- und Trendrelevanz (skalierbar via CONFIG-Schwellen)\n",
    "    Returns: DataFrame mit gefilterten News + Relevanz-Scores\n",
    "    \"\"\"\n",
    "    df_copy = df_news.copy()\n",
    "\n",
    "    # Initialisiere Spalten f√ºr Relevanz\n",
    "    df_copy[\"kurs_relevanz\"] = 0.0\n",
    "    df_copy[\"trend_relevanz\"] = 0.0\n",
    "    df_copy[\"trend_richtung\"] = \"neutral\"\n",
    "\n",
    "    print(f\"üîç Bewerte Relevanz von {len(df_copy)} News-Eintr√§gen...\")\n",
    "\n",
    "    # Iteriere √ºber alle News (skalierbar f√ºr gro√üe Datens√§tze)\n",
    "    for idx, row in df_copy.iterrows():\n",
    "        ticker = row[CONFIG[\"NEWS_SPALTEN\"][\"ticker\"]]\n",
    "        # Kombiniere Title + Keyword f√ºr bessere Bewertung\n",
    "        news_text = f\"{row[CONFIG['NEWS_SPALTEN']['title']]} {row[CONFIG['NEWS_SPALTEN']['keyword']]}\"\n",
    "\n",
    "        # Bewerte Relevanz\n",
    "        kurs_rel, trend_rel, trend_rich = bewerte_relevanz(news_text, ticker)\n",
    "\n",
    "        # Speichere Ergebnisse\n",
    "        df_copy.at[idx, \"kurs_relevanz\"] = kurs_rel\n",
    "        df_copy.at[idx, \"trend_relevanz\"] = trend_rel\n",
    "        df_copy.at[idx, \"trend_richtung\"] = trend_rich\n",
    "\n",
    "    # Filter nach Mindestschwellen (skalierbar via CONFIG)\n",
    "    df_gefiltert = df_copy[\n",
    "        (df_copy[\"kurs_relevanz\"] >= CONFIG[\"MIN_KURS_RELEVANZ\"]) &\n",
    "        (df_copy[\"trend_relevanz\"] >= CONFIG[\"MIN_TREND_RELEVANZ\"])\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    print(f\"‚úÖ {len(df_gefiltert)} relevante News (von {len(df_copy)} total)\")\n",
    "    return df_gefiltert\n",
    "\n",
    "# --- 5. Neutralisierung der News (optimiert f√ºr Finanztexte) ---\n",
    "def neutralisiere_news_text(news_text, ticker):\n",
    "    \"\"\"\n",
    "    Neutralisiert News-Text (entfernt Bias/emotionale Sprache)\n",
    "    Skalierbar f√ºr beliebige Ticker\n",
    "    \"\"\"\n",
    "    if not isinstance(news_text, str) or len(news_text) < 10:\n",
    "        return news_text\n",
    "\n",
    "    client = init_gemini_client()\n",
    "    system_prompt = f\"\"\"\n",
    "    Du bist ein neutraler Finanzjournalist. Rewrite den folgenden Text √ºber den Aktien-Ticker {ticker}:\n",
    "    - Entferne alle emotionalen Ausdr√ºcke (z.B. \"katastrophal\", \"erstaunlich\", \"fantastisch\")\n",
    "    - Entferne subjektive Meinungen (z.B. \"Experten glauben, dass...\")\n",
    "    - Behalte alle objektiven Fakten bei (Produktionsbr√ºche, Umsatzzahlen, Investitionen etc.)\n",
    "    - Halte dich strikt an den Originalinhalt (keine Erweiterungen/Reduzierungen)\n",
    "    - Nutze pr√§zise, faktenbasierte Sprache (typisch f√ºr Finanzberichte)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate_content(\n",
    "            contents=news_text[:3000],\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=CONFIG[\"TEMPERATUR\"],\n",
    "                max_output_tokens=1500\n",
    "            ),\n",
    "            system_instruction=system_prompt\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fehler bei Neutralisierung: {e}\")\n",
    "        return news_text\n",
    "\n",
    "def neutralisiere_alle_news(df_gefiltert):\n",
    "    \"\"\"\n",
    "    Neutralisiert alle gefilterten News (skalierbar)\n",
    "    \"\"\"\n",
    "    df_copy = df_gefiltert.copy()\n",
    "    df_copy[\"neutraler_text\"] = df_copy.apply(\n",
    "        lambda row: neutralisiere_news_text(\n",
    "            f\"{row[CONFIG['NEWS_SPALTEN']['title']]} {row[CONFIG['NEWS_SPALTEN']['keyword']]}\",\n",
    "            row[CONFIG[\"NEWS_SPALTEN\"][\"ticker\"]]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    print(\"‚úÖ Alle relevanten News neutralisiert\")\n",
    "    return df_copy\n",
    "\n",
    "# --- 6. Zusammenfassung pro Ticker (skalierbar) ---\n",
    "def zusammenfasse_news_pro_ticker(df_verarbeitet):\n",
    "    \"\"\"\n",
    "    Erstellt strukturierte Zusammenfassungen pro Ticker (skalierbar)\n",
    "    Returns: Dictionary mit Zusammenfassungen\n",
    "    \"\"\"\n",
    "    client = init_gemini_client()\n",
    "    zusammenfassungen = {}\n",
    "\n",
    "    for ticker in CONFIG[\"AKTIEN_TICKER\"]:\n",
    "        df_ticker = df_verarbeitet[df_verarbeitet[CONFIG[\"NEWS_SPALTEN\"][\"ticker\"]] == ticker]\n",
    "\n",
    "        if len(df_ticker) == 0:\n",
    "            zusammenfassungen[ticker] = \"Keine relevanten News gefunden.\"\n",
    "            continue\n",
    "\n",
    "        # Kombiniere neutralisierte Texte\n",
    "        combined_text = \"\\n---\\n\".join(df_ticker[\"neutraler_text\"].tolist())[:5000]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Erstelle eine strukturierte Zusammenfassung der relevanten News f√ºr {ticker} (letzte 6 Monate):\n",
    "\n",
    "        STRUKTUR:\n",
    "        1. Kursrelevante Ereignisse (z.B. Produktionsbr√ºche, Umsatzmeldungen, Investitionen)\n",
    "        2. Trendauswirkungen (Welche Richtung (up/down/neutral) wird beeinflusst? Warum?)\n",
    "        3. Wichtige Fakten (keine Meinungen, nur objekte Informationen)\n",
    "\n",
    "        ANWEISUNGEN:\n",
    "        - Maximal 300 W√∂rter pro Abschnitt\n",
    "        - Nur Fakten aus den bereitgestellten Texten\n",
    "        - Neutral und faktenbasiert\n",
    "\n",
    "        NEWS-TEXTE:\n",
    "        {combined_text}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.generate_content(prompt)\n",
    "            zusammenfassungen[ticker] = response.text\n",
    "            print(f\"‚úÖ Zusammenfassung f√ºr {ticker} erstellt\")\n",
    "        except Exception as e:\n",
    "            zusammenfassungen[ticker] = f\"Fehler bei Zusammenfassung: {str(e)}\"\n",
    "\n",
    "    return zusammenfassungen\n",
    "\n",
    "# --- 7. Speicherfunktionen (CSV + JSON, skalierbar) ---\n",
    "def speichere_ergebnisse(df_verarbeitet, zusammenfassungen):\n",
    "    \"\"\"\n",
    "    Speichert die verarbeiteten Daten als CSV und JSON (skalierbar)\n",
    "    \"\"\"\n",
    "    # Speichere verarbeitete News als CSV\n",
    "    csv_pfad = \"verarbeitete_aktien_news.csv\"\n",
    "    df_verarbeitet.to_csv(csv_pfad, index=False, encoding=\"utf-8\")\n",
    "    print(f\"üìÑ Verarbeitete News gespeichert: {csv_pfad}\")\n",
    "\n",
    "    # Speichere Zusammenfassungen als JSON\n",
    "    json_pfad = \"aktien_news_zusammenfassungen.json\"\n",
    "    with open(json_pfad, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(zusammenfassungen, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"üìÑ Zusammenfassungen gespeichert: {json_pfad}\")\n",
    "\n",
    "    # Optional: Speichere Kursdaten als CSV (f√ºr Nachverarbeitung)\n",
    "    try:\n",
    "        df_kurs = lade_alle_kursdaten()\n",
    "        kurs_csv_pfad = \"alle_aktien_kursdaten.csv\"\n",
    "        df_kurs.to_csv(kurs_csv_pfad, index=False, encoding=\"utf-8\")\n",
    "        print(f\"üìÑ Kursdaten gespeichert: {kurs_csv_pfad}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Kursdaten konnten nicht gespeichert werden: {e}\")\n",
    "\n",
    "# --- 8. Hauptpipeline (vollst√§ndig & skalierbar) ---\n",
    "def hauptverarbeitung():\n",
    "    \"\"\"\n",
    "    Vollst√§ndige Pipeline:\n",
    "    1. Daten laden ‚Üí 2. Relevanzfilter ‚Üí 3. Neutralisierung ‚Üí 4. Zusammenfassung ‚Üí 5. Speichern\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starte Hauptverarbeitung...\")\n",
    "\n",
    "    # Schritt 1: Daten laden\n",
    "    df_news = lade_news_daten()\n",
    "    _ = lade_alle_kursdaten()  # Nur zur Pr√ºfung, ob Kursdaten existieren\n",
    "\n",
    "    # Schritt 2: Filter nach Relevanz (Kurs + Trend)\n",
    "    df_gefiltert = filter_news_nach_relevanz(df_news)\n",
    "\n",
    "    if len(df_gefiltert) == 0:\n",
    "        print(\"‚ö†Ô∏è Keine relevanten News gefunden!\")\n",
    "        return\n",
    "\n",
    "    # Schritt 3: Neutralisierung\n",
    "    df_verarbeitet = neutralisiere_alle_news(df_gefiltert)\n",
    "\n",
    "    # Schritt 4: Zusammenfassung pro Ticker\n",
    "    zusammenfassungen = zusammenfasse_news_pro_ticker(df_verarbeitet)\n",
    "\n",
    "    # Schritt 5: Ergebnisse speichern\n",
    "    speichere_ergebnisse(df_verarbeitet, zusammenfassungen)\n",
    "\n",
    "    # Ausgabe der Zusammenfassungen\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìù FINALE ZUSAMMENFASSUNGEN PRO TICKER\")\n",
    "    print(\"=\"*50)\n",
    "    for ticker, summary in zusammenfassungen.items():\n",
    "        print(f\"\\n--- {ticker} ---\")\n",
    "        print(summary)\n",
    "\n",
    "    return df_verarbeitet, zusammenfassungen\n",
    "\n",
    "# --- 9. Ausf√ºhrung der Pipeline ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Starte die vollst√§ndige Verarbeitung\n",
    "    df_verarbeitet, zusammenfassungen = hauptverarbeitung()"
   ],
   "id": "2efb692fc096c32e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3add092865bd4564"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eb6f0d4520524ed5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
