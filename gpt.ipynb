{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-07T01:17:47.332858Z",
     "start_time": "2025-12-07T01:17:30.007682Z"
    }
   },
   "source": [
    "# API-Schlüssel für Google Generative AI (Gemini) einfügen\n",
    "API_KEY = \"AIzaSyCZhsJYSDnsNC3-LIeIhVZzodiWvUIvW3M\"\n",
    "\n",
    "# Installation der Google Generative AI Bibliothek (falls nicht bereits installiert)\n",
    "!pip install google-generativeai\n",
    "\n",
    "# Installation weiterer benötigter Bibliotheken\n",
    "!pip install beautifulsoup4 requests pandas matplotlib\n",
    "\n",
    "# Imports\n",
    "import google.generativeai as palm\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta, date\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-generativeai) (2.12.5)\n",
      "Requirement already satisfied: tqdm in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: colorama in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: requests in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: numpy>=1.26.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in h:\\phython prjekts\\ai_agentic_team1\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:17:47.442246Z",
     "start_time": "2025-12-07T01:17:47.430479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# API-Key konfigurieren\n",
    "palm.configure(api_key=API_KEY)\n",
    "\n",
    "# Modell-Name (abhängig von der Verfügbarkeit; hier verwenden wir das Chatmodell von Gemini/Bison)\n",
    "MODEL_NAME = \"models/chat-bison-001\"  # Alternativ: \"models/text-bison-001\" oder ein spezifisches Gemini-Modell, falls verfügbar\n",
    "\n",
    "# Optionale Parameter für die Textgenerierung\n",
    "generation_params = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"temperature\": 0.2,        # geringe Temperature für fokussierte/gut strukturierte Antworten\n",
    "    \"candidate_count\": 1\n",
    "}\n"
   ],
   "id": "4fec3d06898ff69e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:18:34.303632Z",
     "start_time": "2025-12-07T01:18:34.068244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Laden der News-Daten\n",
    "news_df = pd.read_csv(\"News data/news_6m_finnhub_newsapi.csv\")\n",
    "print(\"Anzahl geladener News:\", len(news_df))\n",
    "print(\"Spalten:\", news_df.columns.tolist())\n",
    "# Ersten Eintrag anzeigen\n",
    "print(\"\\nBeispielhafter News-Eintrag:\")\n",
    "display(news_df.head(1))\n"
   ],
   "id": "7c9caf5184d8cbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl geladener News: 709\n",
      "Spalten: ['ticker', 'provider', 'channel', 'title', 'url', 'published_at_utc', 'news_source', 'keyword']\n",
      "\n",
      "Beispielhafter News-Eintrag:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  ticker provider channel                                              title  \\\n",
       "0   NVDA  finnhub    news  Uber: Nvidia Partnership Allays Any Fears Of T...   \n",
       "\n",
       "                                                 url      published_at_utc  \\\n",
       "0  https://finnhub.io/api/news?id=7337f668cc034c0...  2025-11-20T04:05:05Z   \n",
       "\n",
       "    news_source keyword  \n",
       "0  SeekingAlpha     NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>provider</th>\n",
       "      <th>channel</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_at_utc</th>\n",
       "      <th>news_source</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>finnhub</td>\n",
       "      <td>news</td>\n",
       "      <td>Uber: Nvidia Partnership Allays Any Fears Of T...</td>\n",
       "      <td>https://finnhub.io/api/news?id=7337f668cc034c0...</td>\n",
       "      <td>2025-11-20T04:05:05Z</td>\n",
       "      <td>SeekingAlpha</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:18:37.022612Z",
     "start_time": "2025-12-07T01:18:36.998763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Auf die ersten 20 Artikel beschränken\n",
    "news_top20 = news_df.iloc[:20].copy().reset_index(drop=True)\n",
    "print(\"Erste 20 Artikel geladen.\")\n",
    "# Optionale Anzeige der Titel dieser 20 Artikel zur Übersicht\n",
    "for i, row in news_top20.iterrows():\n",
    "    print(f\"{i+1}. [{row['ticker']}] {row['title']} ({row['published_at_utc']})\")\n"
   ],
   "id": "8abd1de61ebe2cd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erste 20 Artikel geladen.\n",
      "1. [NVDA] Uber: Nvidia Partnership Allays Any Fears Of The AV Threat (Rating Upgrade) (2025-11-20T04:05:05Z)\n",
      "2. [NVDA] Arista Networks: More Of A 2026 Opportunity Due To The Lagging Effect (2025-11-20T00:55:01Z)\n",
      "3. [NVDA] NVIDIA Corporation (NVDA) Q3 2026 Earnings Call Transcript (2025-11-19T22:13:24Z)\n",
      "4. [NVDA] NVIDIA Corporation 2026 Q3 - Results - Earnings Call Presentation (2025-11-19T22:01:31Z)\n",
      "5. [NVDA] Nvidia: I Was Wrong, But I'm Not A Buyer Here (2025-11-19T19:00:24Z)\n",
      "6. [NVDA] Nvidia: Stellar Q3 And Sold-Out Blackwell Signal Explosive AI Demand (2025-11-19T18:45:03Z)\n",
      "7. [NVDA] Nvidia: Stellar Q3 And Explosive Guidance Make The Bear Case Harder (Upgrade) (2025-11-19T18:30:50Z)\n",
      "8. [NVDA] Nvidia: Seismic Quarter Redefining AI Markets (2025-11-19T18:15:59Z)\n",
      "9. [NVDA] Nvidia: Huge Growth, Breath Of Relief (2025-11-19T18:00:32Z)\n",
      "10. [NVDA] Qualcomm to open engineering hub in Saudi Arabia, part of a series of AI deals in kingdom (2025-11-19T18:00:08Z)\n",
      "11. [NVDA] 3 Best Performing AI Stocks In the Nasdaq Composite (2025-11-19T17:55:53Z)\n",
      "12. [NVDA] US Equity Indexes Mixed as Big-Tech Helps Lift Communication Services, Technology (2025-11-19T17:48:26Z)\n",
      "13. [NVDA] Stock Market Today: Indexes Mixed Ahead Of Nvidia Earnings; SEO Stock Soars On Adobe News (Live Coverage) (2025-11-19T17:47:43Z)\n",
      "14. [NVDA] Nvidia: Keep Buying After Strong Q3 Results (2025-11-19T17:45:56Z)\n",
      "15. [NVDA] October Readers Identified 10 Ideal \"Safer\" Dividends In 39 Dogs (2025-11-19T17:43:09Z)\n",
      "16. [NVDA] Pound falters as soft inflation tees up rate cut (2025-11-19T17:36:39Z)\n",
      "17. [NVDA] 3 Growth Companies Insiders Are Betting On (2025-11-19T17:36:09Z)\n",
      "18. [NVDA] Before you buy Nvidia, here’s what one expert wants you to know (2025-11-19T17:35:38Z)\n",
      "19. [NVDA] These Stocks Are Moving the Most Today: Nvidia, Lowe’s, Target, Semrush, Constellation Energy, Plug Power, and More (2025-11-19T13:44:00Z)\n",
      "20. [NVDA] Nvidia Stock: Time To Sell (2025-11-19T17:30:59Z)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:19:45.705441Z",
     "start_time": "2025-12-07T01:19:45.447270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hilfsfunktion zum Laden der Kursdaten-Dateien (Überspringen der Header-Zeilen)\n",
    "def load_stock_data(csv_path):\n",
    "    # Die CSV hat 3 Header-Zeilen, wir überspringen diese\n",
    "    col_names = [\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "    df = pd.read_csv(csv_path, skiprows=3, names=col_names)\n",
    "    # Datums-Spalte in echtes Datum konvertieren\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df\n",
    "\n",
    "# Aktienkursdaten pro Ticker laden\n",
    "tickers = [\"NVDA\", \"TSLA\", \"ASML\", \"META\", \"AMZN\"]\n",
    "stock_data = {}\n",
    "for ticker in tickers:\n",
    "    filename = f\"Stockcorse_as_csv/{ticker}_6monatealles.csv\"\n",
    "    try:\n",
    "        df = load_stock_data(filename)\n",
    "        stock_data[ticker] = df\n",
    "        print(f\"{ticker}: {len(df)} Kurs-Datensätze geladen, Zeitraum {df['Date'].min().date()} bis {df['Date'].max().date()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warnung: Datei {filename} nicht gefunden.\")\n"
   ],
   "id": "cc4e40f849f6363f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVDA: 127 Kurs-Datensätze geladen, Zeitraum 2025-06-05 bis 2025-12-04\n",
      "TSLA: 127 Kurs-Datensätze geladen, Zeitraum 2025-06-05 bis 2025-12-04\n",
      "ASML: 127 Kurs-Datensätze geladen, Zeitraum 2025-06-05 bis 2025-12-04\n",
      "META: 127 Kurs-Datensätze geladen, Zeitraum 2025-06-05 bis 2025-12-04\n",
      "AMZN: 127 Kurs-Datensätze geladen, Zeitraum 2025-06-05 bis 2025-12-04\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:19:53.285869Z",
     "start_time": "2025-12-07T01:19:53.267335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_article_content(url):\n",
    "    \"\"\"\n",
    "    Ruft die gegebene URL auf und gibt den Haupttext des Artikels zurück.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Abrufen von {url}: {e}\")\n",
    "        return None\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Fehler: HTTP {response.status_code} für URL {url}\")\n",
    "        return None\n",
    "\n",
    "    html = response.text\n",
    "    # HTML parsen mit BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Alle Absatz-Texte sammeln\n",
    "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "    # Falls der extrahierte Text sehr kurz ist, evtl. Alternativen prüfen (z.B. <article>-Tag Inhalte)\n",
    "    if len(text) < 100:\n",
    "        # Versuchen wir, den Inhalt eines <article> oder <div> mit main-Content zu holen\n",
    "        article_tag = soup.find('article')\n",
    "        if article_tag:\n",
    "            text = article_tag.get_text()\n",
    "        else:\n",
    "            main_div = soup.find('div', {'class': 'article'})\n",
    "            if main_div:\n",
    "                text = main_div.get_text()\n",
    "    # Kurzen Text oder Fehlschlag behandeln\n",
    "    if len(text.strip()) == 0:\n",
    "        print(\"Warnung: Kein Inhalt extrahiert von\", url)\n",
    "        return None\n",
    "    return text.strip()\n"
   ],
   "id": "32fc24a6f119d5ea",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:19:59.925430Z",
     "start_time": "2025-12-07T01:19:56.161231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test: Ersten Artikelinhalt abrufen (optional)\n",
    "test_url = news_top20.loc[0, 'url']\n",
    "print(\"Teste Abruf für URL:\", test_url)\n",
    "sample_text = fetch_article_content(test_url)\n",
    "if sample_text:\n",
    "    print(\"Auszug aus dem Artikeltext:\\n\", sample_text[:500], \"...\\n\")\n",
    "else:\n",
    "    print(\"Artikeltext konnte nicht abgerufen werden.\")\n"
   ],
   "id": "53a7a45561955e23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste Abruf für URL: https://finnhub.io/api/news?id=7337f668cc034c00410c137760d595e1d967ffdeb276789029ffe1d17f70b959\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=7337f668cc034c00410c137760d595e1d967ffdeb276789029ffe1d17f70b959\n",
      "Artikeltext konnte nicht abgerufen werden.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:21:01.710751Z",
     "start_time": "2025-12-07T01:21:01.689260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def analyze_article(title, pub_date, content):\n",
    "    \"\"\"\n",
    "    Analysiert einen Artikel mittels generativer KI und gibt ein Dict mit Ticker, Zeitrahmen und Begründung zurück.\n",
    "    \"\"\"\n",
    "    # Formatieren des Datums für den Prompt (z.B. \"2025-11-20\")\n",
    "    pub_date_str = pub_date.strftime(\"%Y-%m-%d\")\n",
    "    # Prompt für das Modell erstellen\n",
    "    prompt = (\n",
    "        \"Lies den folgenden Nachrichtenartikel und beantworte danach:\\n\"\n",
    "        f\"Titel: {title}\\n\"\n",
    "        f\"Veröffentlicht am: {pub_date_str}\\n\"\n",
    "        f\"Inhalt:\\n{content}\\n\\n\"\n",
    "        \"Bestimme, welche der folgenden Aktien dieser Artikel hauptsächlich betrifft: NVDA, TSLA, ASML, META oder AMZN.\\n\"\n",
    "        \"Gib außerdem an, über welchen Zeitraum (relativ zum Veröffentlichungsdatum) die Nachricht voraussichtlich den Aktienkurs beeinflusst. \"\n",
    "        \"Drücke den Zeitraum in Tagen nach der Veröffentlichung aus (z.B. 0-3 Tage nach Veröffentlichung, 1 Tag vor bis 5 Tage nach Veröffentlichung) oder bis zu einem bestimmten Ereignis (z.B. bis Ende des Quartals). \"\n",
    "        \"Begründe kurz diese Einschätzung basierend auf dem Artikelinhalt.\\n\"\n",
    "        \"Antworte **nur** in folgendem JSON-Format:\\n\"\n",
    "        \"{ \\\"ticker\\\": <Ticker>, \\\"timeframe\\\": <Zeitrahmen>, \\\"reason\\\": <Begründung> }\\n\"\n",
    "    )\n",
    "    # Aufruf des generativen Modells\n",
    "    try:\n",
    "        response = palm.generate_text(prompt=prompt, **generation_params)\n",
    "    except Exception as e:\n",
    "        print(\"Fehler bei Aufruf von Gemini API:\", e)\n",
    "        return None\n",
    "\n",
    "    if hasattr(response, 'result'):\n",
    "        raw_answer = response.result\n",
    "    elif isinstance(response, str):\n",
    "        raw_answer = response  # falls direkt Text zurückkommt\n",
    "    else:\n",
    "        # Unterschiedliche Library-Versionen könnten andere Attribute haben\n",
    "        raw_answer = str(response)\n",
    "\n",
    "    # JSON-Antwort parsen\n",
    "    answer_str = raw_answer.strip()\n",
    "    # Manchmal können zusätzliche Texte vor/nach JSON kommen - versuchen wir, JSON-Teil herauszufiltern\n",
    "    if answer_str.find('{') != -1:\n",
    "        answer_str = answer_str[answer_str.find('{'): answer_str.rfind('}')+1]\n",
    "    try:\n",
    "        result = json.loads(answer_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Konnte JSON nicht parsen. Rohantwort:\\n\", raw_answer)\n",
    "        return None\n",
    "    return result\n"
   ],
   "id": "2e5d11515dfb82f7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:21:56.237361Z",
     "start_time": "2025-12-07T01:21:05.872620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "analysis_results = []  # Liste für Ergebnisse\n",
    "\n",
    "for idx, row in news_top20.iterrows():\n",
    "    title = row['title']\n",
    "    ticker_hint = row['ticker']  # Ticker aus den Daten (als Hinweis, sollte mit Analyse übereinstimmen)\n",
    "    pub_date = pd.to_datetime(row['published_at_utc'])\n",
    "    url = row['url']\n",
    "    print(f\"\\nArtikel {idx+1}: {title}\\nURL: {url}\")\n",
    "\n",
    "    # Artikelinhalt abrufen\n",
    "    content = fetch_article_content(url)\n",
    "    if content is None:\n",
    "        print(\"Artikelinhalt nicht verfügbar, überspringe.\")\n",
    "        continue\n",
    "\n",
    "    # Begrenzen wir extrem lange Inhalte (falls z.B. Kommentare angehängt sind), um Token zu sparen\n",
    "    if len(content) > 10000:\n",
    "        content = content[:10000]  # ersten 10000 Zeichen behalten\n",
    "        print(\"(Inhalt gekürzt für Analyse)\")\n",
    "\n",
    "    # LLM-Analyse durchführen\n",
    "    result = analyze_article(title, pub_date, content)\n",
    "    if result is None:\n",
    "        print(\"Analyse fehlgeschlagen oder kein Ergebnis.\")\n",
    "        continue\n",
    "\n",
    "    # Parsed result enthält 'ticker', 'timeframe', 'reason'\n",
    "    ticker_pred = result.get('ticker', '').upper()\n",
    "    timeframe = result.get('timeframe', '')\n",
    "    reason = result.get('reason', '')\n",
    "    # Falls ticker_pred leer oder nicht einer der erwarteten, ggf. fallback auf vorhandenen ticker_hint\n",
    "    if ticker_pred not in tickers:\n",
    "        ticker_pred = ticker_hint  # zur Sicherheit\n",
    "    # Ergebnis speichern\n",
    "    analysis_results.append({\n",
    "        \"title\": title,\n",
    "        \"published\": pub_date,\n",
    "        \"ticker\": ticker_pred,\n",
    "        \"timeframe_text\": timeframe,\n",
    "        \"reason\": reason\n",
    "    })\n",
    "    # Ausgabe der Analyse\n",
    "    print(f\"➡️ Erkanntes Unternehmen: {ticker_pred}\")\n",
    "    print(f\"➡️ Geschätzter Zeitraum: {timeframe}\")\n",
    "    print(f\"➡️ Begründung: {reason}\\n\")\n"
   ],
   "id": "a3ed0ce1c1d5baf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artikel 1: Uber: Nvidia Partnership Allays Any Fears Of The AV Threat (Rating Upgrade)\n",
      "URL: https://finnhub.io/api/news?id=7337f668cc034c00410c137760d595e1d967ffdeb276789029ffe1d17f70b959\n",
      "Fehler bei Aufruf von Gemini API: module 'google.generativeai' has no attribute 'generate_text'\n",
      "Analyse fehlgeschlagen oder kein Ergebnis.\n",
      "\n",
      "Artikel 2: Arista Networks: More Of A 2026 Opportunity Due To The Lagging Effect\n",
      "URL: https://finnhub.io/api/news?id=696f3cf9f4908cd3c75565e7a579abc4e327a0f759fec8c449f663f59144246e\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=696f3cf9f4908cd3c75565e7a579abc4e327a0f759fec8c449f663f59144246e\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 3: NVIDIA Corporation (NVDA) Q3 2026 Earnings Call Transcript\n",
      "URL: https://finnhub.io/api/news?id=afd80a18e3ace0b3a8c58a1df985cba415cf51e6e1e7895c9d97aba0c360f2dd\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=afd80a18e3ace0b3a8c58a1df985cba415cf51e6e1e7895c9d97aba0c360f2dd\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 4: NVIDIA Corporation 2026 Q3 - Results - Earnings Call Presentation\n",
      "URL: https://finnhub.io/api/news?id=b8b4c837fb061a8f20f04c1eef174206e879d76e9789423c8f65ab9644a7935a\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=b8b4c837fb061a8f20f04c1eef174206e879d76e9789423c8f65ab9644a7935a\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 5: Nvidia: I Was Wrong, But I'm Not A Buyer Here\n",
      "URL: https://finnhub.io/api/news?id=dc4e30b20ccbf020760387db90d3ed35bd48fe38b1a3a1f724ccd17bf5795bee\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=dc4e30b20ccbf020760387db90d3ed35bd48fe38b1a3a1f724ccd17bf5795bee\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 6: Nvidia: Stellar Q3 And Sold-Out Blackwell Signal Explosive AI Demand\n",
      "URL: https://finnhub.io/api/news?id=47d795b3094d65bf784d475e4f9f6fa5f04923ed1802c8f14b4d6c0fbe0ae82a\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=47d795b3094d65bf784d475e4f9f6fa5f04923ed1802c8f14b4d6c0fbe0ae82a\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 7: Nvidia: Stellar Q3 And Explosive Guidance Make The Bear Case Harder (Upgrade)\n",
      "URL: https://finnhub.io/api/news?id=99b5d344ec3610dab304f0f10fbe66a0b9e03e1c1d091944a5a44cd32225fe49\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=99b5d344ec3610dab304f0f10fbe66a0b9e03e1c1d091944a5a44cd32225fe49\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 8: Nvidia: Seismic Quarter Redefining AI Markets\n",
      "URL: https://finnhub.io/api/news?id=900293153dfea2d7eba07ecd115ec662d47c2dfff3c91ee873284d27b6c0e7b2\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=900293153dfea2d7eba07ecd115ec662d47c2dfff3c91ee873284d27b6c0e7b2\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 9: Nvidia: Huge Growth, Breath Of Relief\n",
      "URL: https://finnhub.io/api/news?id=a6199212d7f3f73b4fa8718851a77843c964cecaede2592f508c996b63b46991\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=a6199212d7f3f73b4fa8718851a77843c964cecaede2592f508c996b63b46991\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 10: Qualcomm to open engineering hub in Saudi Arabia, part of a series of AI deals in kingdom\n",
      "URL: https://finnhub.io/api/news?id=49c541f0c8df928634351820e1f1d7ebfdbc0ad4dec3c4d270b8510e254ce483\n",
      "Fehler: HTTP 429 für URL https://finnhub.io/api/news?id=49c541f0c8df928634351820e1f1d7ebfdbc0ad4dec3c4d270b8510e254ce483\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 11: 3 Best Performing AI Stocks In the Nasdaq Composite\n",
      "URL: https://finnhub.io/api/news?id=71aa3b57d9508887dc2e6cdb0c88252d4cde370d8740369b317b3fa32d75365b\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=71aa3b57d9508887dc2e6cdb0c88252d4cde370d8740369b317b3fa32d75365b\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 12: US Equity Indexes Mixed as Big-Tech Helps Lift Communication Services, Technology\n",
      "URL: https://finnhub.io/api/news?id=329fbbb43713a1dfe0c960d6c69de77787f41ac8944399f5009558af9009ef00\n",
      "Fehler: HTTP 429 für URL https://finnhub.io/api/news?id=329fbbb43713a1dfe0c960d6c69de77787f41ac8944399f5009558af9009ef00\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 13: Stock Market Today: Indexes Mixed Ahead Of Nvidia Earnings; SEO Stock Soars On Adobe News (Live Coverage)\n",
      "URL: https://finnhub.io/api/news?id=a37e15756976acfd118690fa8294a31ea136795768b8607154974e6107825195\n",
      "Fehler: HTTP 429 für URL https://finnhub.io/api/news?id=a37e15756976acfd118690fa8294a31ea136795768b8607154974e6107825195\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 14: Nvidia: Keep Buying After Strong Q3 Results\n",
      "URL: https://finnhub.io/api/news?id=8d8a3f57cd590c60b420ed58659c02f80770477b6d2c756799ae02f500363069\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=8d8a3f57cd590c60b420ed58659c02f80770477b6d2c756799ae02f500363069\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 15: October Readers Identified 10 Ideal \"Safer\" Dividends In 39 Dogs\n",
      "URL: https://finnhub.io/api/news?id=e7780ded97afaf334f2f03e1cbaea39f49578a914aa9b9c956ba4c97407643db\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=e7780ded97afaf334f2f03e1cbaea39f49578a914aa9b9c956ba4c97407643db\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 16: Pound falters as soft inflation tees up rate cut\n",
      "URL: https://finnhub.io/api/news?id=e291184ec538dcdbd7107e67b0f0e1f428d6b2b5bc4774ff2b229dcead9efabd\n",
      "Fehler: HTTP 429 für URL https://finnhub.io/api/news?id=e291184ec538dcdbd7107e67b0f0e1f428d6b2b5bc4774ff2b229dcead9efabd\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 17: 3 Growth Companies Insiders Are Betting On\n",
      "URL: https://finnhub.io/api/news?id=7eabe7ae9b358a042d8e7f68ac5ee3dc130634321c531eca11376c8a06aeca0c\n",
      "Fehler: HTTP 429 für URL https://finnhub.io/api/news?id=7eabe7ae9b358a042d8e7f68ac5ee3dc130634321c531eca11376c8a06aeca0c\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 18: Before you buy Nvidia, here’s what one expert wants you to know\n",
      "URL: https://finnhub.io/api/news?id=f3ee05980f6eaaa6972a6ca6f6931ecdb746ceb260b6e13c803321c45e8a58a8\n",
      "Fehler: HTTP 429 für URL https://finnhub.io/api/news?id=f3ee05980f6eaaa6972a6ca6f6931ecdb746ceb260b6e13c803321c45e8a58a8\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 19: These Stocks Are Moving the Most Today: Nvidia, Lowe’s, Target, Semrush, Constellation Energy, Plug Power, and More\n",
      "URL: https://finnhub.io/api/news?id=2e263945ee833f015736be7b0816aedfee0d4ebf3841ab2d755e3cddf4ea1340\n",
      "Fehler: HTTP 429 für URL https://finnhub.io/api/news?id=2e263945ee833f015736be7b0816aedfee0d4ebf3841ab2d755e3cddf4ea1340\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n",
      "\n",
      "Artikel 20: Nvidia Stock: Time To Sell\n",
      "URL: https://finnhub.io/api/news?id=2381891bde37860a95f6f0344cbd057807726a35090ef87df259f11f68c8d253\n",
      "Fehler: HTTP 403 für URL https://finnhub.io/api/news?id=2381891bde37860a95f6f0344cbd057807726a35090ef87df259f11f68c8d253\n",
      "Artikelinhalt nicht verfügbar, überspringe.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:22:04.474203Z",
     "start_time": "2025-12-07T01:22:04.436721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def get_quarter_end(dt):\n",
    "    \"\"\"Hilfsfunktion: gibt das Enddatum des Quartals zurück, in dem dt liegt.\"\"\"\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    if month <= 3:\n",
    "        return date(year, 3, 31)\n",
    "    elif month <= 6:\n",
    "        return date(year, 6, 30)\n",
    "    elif month <= 9:\n",
    "        return date(year, 9, 30)\n",
    "    else:\n",
    "        return date(year, 12, 31)\n",
    "\n",
    "def interpret_timeframe(pub_date, timeframe_str):\n",
    "    \"\"\"Interpretiert den Zeitfenster-String und gibt Start- und Enddatum zurück.\"\"\"\n",
    "    pub_date_date = pub_date.date()  # nur Datumsteil\n",
    "    s = timeframe_str.lower()\n",
    "    start_date = pub_date_date\n",
    "    end_date = pub_date_date\n",
    "    # Fall 1: Angabe \"X-Y tage nach\"\n",
    "    if \"tage nach\" in s or \"days after\" in s:\n",
    "        # Bereich nach Veröffentlichung\n",
    "        # Beispiel: \"0-3 tage nach\" oder \"0-3 days after\"\n",
    "        if '-' in s:\n",
    "            # Extrahiere Zahlen vor und nach dem Bindestrich\n",
    "            parts = s.split('-')\n",
    "            try:\n",
    "                start_offset = int(parts[0].strip())\n",
    "            except:\n",
    "                start_offset = 0\n",
    "            # Suche nach der zweiten Zahl in parts[1]\n",
    "            import re\n",
    "            match = re.search(r'(\\d+)', parts[1])\n",
    "            if match:\n",
    "                end_offset = int(match.group(1))\n",
    "            else:\n",
    "                end_offset = 0\n",
    "        else:\n",
    "            # Kein Bindestrich, vielleicht \"X days after\"\n",
    "            match = re.search(r'(\\d+)', s)\n",
    "            if match:\n",
    "                start_offset = 0\n",
    "                end_offset = int(match.group(1))\n",
    "            else:\n",
    "                start_offset = 0\n",
    "                end_offset = 0\n",
    "        start_date = pub_date_date + timedelta(days=start_offset)\n",
    "        end_date = pub_date_date + timedelta(days=end_offset)\n",
    "    # Fall 2: Angabe enthält \"vor\" und \"nach\"\n",
    "    if \"vor\" in s and \"nach\" in s:\n",
    "        # Beispiel: \"1 tag vor bis 5 tage nach\"\n",
    "        # Nimm die erste Zahl als vor, die zweite als nach\n",
    "        numbers = [int(num) for num in re.findall(r'(\\d+)', s)]\n",
    "        if len(numbers) >= 2:\n",
    "            start_date = pub_date_date - timedelta(days=numbers[0])\n",
    "            end_date = pub_date_date + timedelta(days=numbers[1])\n",
    "    # Fall 3: Quartalsende\n",
    "    if \"quartal\" in s or \"quarter\" in s:\n",
    "        # Von Veröffentlichung bis Quartalsende\n",
    "        start_date = pub_date_date\n",
    "        end_date = get_quarter_end(pub_date_date)\n",
    "    # Fall 4: Angabe von Stunden/Minuten -> interpretieren als gleichen Tag\n",
    "    if \"stunde\" in s or \"hour\" in s or \"minute\" in s or \"intraday\" in s:\n",
    "        start_date = pub_date_date\n",
    "        end_date = pub_date_date\n",
    "    # Sicherstellen, dass Start nicht nach Ende (z.B. falls vor/nach gemischt)\n",
    "    if start_date > end_date:\n",
    "        start_date, end_date = end_date, start_date\n",
    "    return start_date, end_date\n",
    "\n",
    "# Wende interpret_timeframe auf alle Ergebnisse an und hole Kursdaten\n",
    "for res in analysis_results:\n",
    "    pub_date = pd.to_datetime(res[\"published\"])\n",
    "    tf_str = str(res[\"timeframe_text\"]) if res[\"timeframe_text\"] else \"\"\n",
    "    start_date, end_date = interpret_timeframe(pub_date, tf_str)\n",
    "    res[\"start_date\"] = start_date\n",
    "    res[\"end_date\"] = end_date\n",
    "    ticker = res[\"ticker\"]\n",
    "    # Filter Kursdaten für diesen Zeitraum (inklusive Start- und Enddatum)\n",
    "    if ticker in stock_data:\n",
    "        df = stock_data[ticker]\n",
    "        mask = (df['Date'].dt.date >= start_date) & (df['Date'].dt.date <= end_date)\n",
    "        res[\"price_data\"] = df[mask].copy()\n",
    "    else:\n",
    "        res[\"price_data\"] = pd.DataFrame()  # kein Datensatz gefunden\n",
    "    # Log-Ausgabe\n",
    "    print(f\"{res['ticker']}: Zeitraum {start_date} bis {end_date}, {len(res['price_data'])} Kurspunkte\")\n"
   ],
   "id": "ae4f9879a562c77c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:22:10.642200Z",
     "start_time": "2025-12-07T01:22:09.779249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot-Einstellungen\n",
    "plt.style.use('seaborn')  # schönerer Plot-Stil\n",
    "figures = []  # falls wir die Figure-Objekte sammeln wollen\n",
    "\n",
    "for res in analysis_results:\n",
    "    ticker = res[\"ticker\"]\n",
    "    title = res[\"title\"]\n",
    "    pub_date = pd.to_datetime(res[\"published\"])\n",
    "    start_date = res[\"start_date\"]\n",
    "    end_date = res[\"end_date\"]\n",
    "    timeframe_text = res[\"timeframe_text\"]\n",
    "    reason = res[\"reason\"]\n",
    "    price_df = res[\"price_data\"]\n",
    "    if price_df is None or price_df.empty:\n",
    "        continue  # überspringen, falls keine Daten\n",
    "\n",
    "    dates = price_df['Date']\n",
    "    prices = price_df['Close']\n",
    "\n",
    "    # Neue Figure erstellen\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(dates, prices, marker='o', linestyle='-')\n",
    "    # Markierung des Veröffentlichungstags\n",
    "    plt.axvline(x=pub_date, color='red', linestyle='--', label=f'News am {pub_date.date()}')\n",
    "    plt.title(f\"{ticker} Kursverlauf ({start_date} bis {end_date})\")\n",
    "    plt.xlabel(\"Datum\")\n",
    "    plt.ylabel(\"Kurs (Close)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Ausgabe der Analyse-Infos zum Artikel\n",
    "    print(f\"**Artikel:** {title}\")\n",
    "    print(f\"**Aktie:** {ticker}\")\n",
    "    print(f\"**Zeitrahmen laut KI:** {timeframe_text}\")\n",
    "    print(f\"**Begründung:** {reason}\\n\")\n"
   ],
   "id": "1ca08294ef67fb20",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mH:\\Phython Prjekts\\AI_Agentic_Team1\\.venv\\Lib\\site-packages\\matplotlib\\style\\core.py:129\u001B[39m, in \u001B[36muse\u001B[39m\u001B[34m(style)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m129\u001B[39m     style = \u001B[43m_rc_params_in_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mH:\\Phython Prjekts\\AI_Agentic_Team1\\.venv\\Lib\\site-packages\\matplotlib\\__init__.py:906\u001B[39m, in \u001B[36m_rc_params_in_file\u001B[39m\u001B[34m(fname, transform, fail_on_error)\u001B[39m\n\u001B[32m    905\u001B[39m rc_temp = {}\n\u001B[32m--> \u001B[39m\u001B[32m906\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_open_file_or_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfd\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    907\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mtry\u001B[39;49;00m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mH:\\Python neu\\Lib\\contextlib.py:141\u001B[39m, in \u001B[36m_GeneratorContextManager.__enter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    140\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mH:\\Phython Prjekts\\AI_Agentic_Team1\\.venv\\Lib\\site-packages\\matplotlib\\__init__.py:883\u001B[39m, in \u001B[36m_open_file_or_url\u001B[39m\u001B[34m(fname)\u001B[39m\n\u001B[32m    882\u001B[39m fname = os.path.expanduser(fname)\n\u001B[32m--> \u001B[39m\u001B[32m883\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m    884\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m f\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'seaborn'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Plot-Einstellungen\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mplt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m.\u001B[49m\u001B[43muse\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mseaborn\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# schönerer Plot-Stil\u001B[39;00m\n\u001B[32m      3\u001B[39m figures = []  \u001B[38;5;66;03m# falls wir die Figure-Objekte sammeln wollen\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m analysis_results:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mH:\\Phython Prjekts\\AI_Agentic_Team1\\.venv\\Lib\\site-packages\\matplotlib\\style\\core.py:131\u001B[39m, in \u001B[36muse\u001B[39m\u001B[34m(style)\u001B[39m\n\u001B[32m    129\u001B[39m         style = _rc_params_in_file(style)\n\u001B[32m    130\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[32m    132\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstyle\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m is not a valid package style, path of style \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    133\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mfile, URL of style file, or library style name (library \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    134\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mstyles are listed in `style.available`)\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m    135\u001B[39m filtered = {}\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m style:  \u001B[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001B[39;00m\n",
      "\u001B[31mOSError\u001B[39m: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:22:21.924298Z",
     "start_time": "2025-12-07T01:22:21.810623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataFrame aus den Ergebnissen erstellen (ohne die Preisdaten selbst, da diese zeitliche Reihen sind)\n",
    "result_df = pd.DataFrame([\n",
    "    {\n",
    "        \"title\": res[\"title\"],\n",
    "        \"published\": res[\"published\"],\n",
    "        \"ticker\": res[\"ticker\"],\n",
    "        \"timeframe\": res[\"timeframe_text\"],\n",
    "        \"reason\": res[\"reason\"],\n",
    "        \"start_date\": res[\"start_date\"],\n",
    "        \"end_date\": res[\"end_date\"]\n",
    "    }\n",
    "    for res in analysis_results\n",
    "])\n",
    "\n",
    "# Speichern als CSV\n",
    "result_df.to_csv(\"news_analysis_results.csv\", index=False)\n",
    "# Speichern als JSON\n",
    "result_df.to_json(\"news_analysis_results.json\", orient=\"records\", date_format=\"iso\", force_ascii=False)\n",
    "\n",
    "print(\"Ergebnisse gespeichert in 'news_analysis_results.csv' und 'news_analysis_results.json'\")\n",
    "print(f\"Anzahl analysierter Artikel: {len(result_df)}\")\n",
    "display(result_df.head(5))\n"
   ],
   "id": "766f4d0078cd8e6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse gespeichert in 'news_analysis_results.csv' und 'news_analysis_results.json'\n",
      "Anzahl analysierter Artikel: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
