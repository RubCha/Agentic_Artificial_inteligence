{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-07T01:29:39.326992Z",
     "start_time": "2025-12-07T01:29:39.309596Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import google.generativeai as genai\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# --- KONFIGURATION ---\n",
    "# Bitte hier deinen Google Gemini API Key eintragen\n",
    "API_KEY = \"AIzaSyCZhsJYSDnsNC3-LIeIhVZzodiWvUIvW3M\"\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# Modell initialisieren (Nutze 'gemini-1.5-flash' für Geschwindigkeit und Kosten oder 'pro' für Tiefe)\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "# Dateipfade (wie von dir definiert)\n",
    "NEWS_PATH = \"News data/news_6m_finnhub_newsapi.csv\"\n",
    "STOCK_PATHS = {\n",
    "    \"NVDA\": \"Stockcorse_as_csv/NVDA_6monatealles.csv\",\n",
    "    \"TSLA\": \"Stockcorse_as_csv/TSLA_6monatealles.csv\",\n",
    "    \"ASML\": \"Stockcorse_as_csv/ASML_6monatealles.csv\",\n",
    "    \"META\": \"Stockcorse_as_csv/META_6monatealles.csv\",\n",
    "    \"AMZN\": \"Stockcorse_as_csv/AMZN_6monatealles.csv\"\n",
    "}\n",
    "\n",
    "print(\"Bibliotheken geladen und Konfiguration gesetzt.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken geladen und Konfiguration gesetzt.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:29:39.500309Z",
     "start_time": "2025-12-07T01:29:39.426819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    # 1. News laden\n",
    "    try:\n",
    "        df_news = pd.read_csv(NEWS_PATH)\n",
    "        # Nimm nur die ersten 20, wie gewünscht\n",
    "        df_news = df_news.head(20)\n",
    "        print(f\"News geladen: {len(df_news)} Artikel.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fehler: Datei {NEWS_PATH} nicht gefunden.\")\n",
    "        return None, None\n",
    "\n",
    "    # 2. Aktiendaten laden\n",
    "    stock_data = {}\n",
    "    for ticker, path in STOCK_PATHS.items():\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            # Wir nehmen an, es gibt eine Spalte 'Date' oder ähnlich.\n",
    "            # Wir konvertieren die erste Spalte zu Datetime und setzen sie als Index.\n",
    "            # Falls deine Spalte anders heißt, muss das hier angepasst werden.\n",
    "            date_col = df.columns[0]\n",
    "            df[date_col] = pd.to_datetime(df[date_col])\n",
    "            df.set_index(date_col, inplace=True)\n",
    "            df.sort_index(inplace=True)\n",
    "            stock_data[ticker] = df\n",
    "            print(f\"Aktienkurse für {ticker} geladen.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden von {ticker}: {e}\")\n",
    "\n",
    "    return df_news, stock_data\n",
    "\n",
    "df_news, stock_data = load_data()"
   ],
   "id": "365c0b634880685e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News geladen: 20 Artikel.\n",
      "Fehler beim Laden von NVDA: Unknown datetime string format, unable to parse: Ticker, at position 0\n",
      "Fehler beim Laden von TSLA: Unknown datetime string format, unable to parse: Ticker, at position 0\n",
      "Fehler beim Laden von ASML: Unknown datetime string format, unable to parse: Ticker, at position 0\n",
      "Fehler beim Laden von META: Unknown datetime string format, unable to parse: Ticker, at position 0\n",
      "Fehler beim Laden von AMZN: Unknown datetime string format, unable to parse: Ticker, at position 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giuseppe\\AppData\\Local\\Temp\\ipykernel_20084\\2850447981.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Giuseppe\\AppData\\Local\\Temp\\ipykernel_20084\\2850447981.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Giuseppe\\AppData\\Local\\Temp\\ipykernel_20084\\2850447981.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Giuseppe\\AppData\\Local\\Temp\\ipykernel_20084\\2850447981.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n",
      "C:\\Users\\Giuseppe\\AppData\\Local\\Temp\\ipykernel_20084\\2850447981.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[date_col] = pd.to_datetime(df[date_col])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:29:39.743058Z",
     "start_time": "2025-12-07T01:29:39.722332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_article_content(url):\n",
    "    \"\"\"Besucht die URL und extrahiert den Text.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Versuche, den Haupttext zu finden (P-Tags sind meistens der Artikel)\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text = \" \".join([p.get_text() for p in paragraphs])\n",
    "            # Kürzen, falls zu lang für Token-Limit (ca. 10.000 Zeichen reichen meist zur Analyse)\n",
    "            return text[:15000]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Scraping Fehler bei {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_with_gemini(article_text, publication_date):\n",
    "    \"\"\"\n",
    "    Analysiert den Text mit Gemini.\n",
    "    Erwartet JSON Output für strukturierte Weiterverarbeitung.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Du bist ein erfahrener Finanzanalyst. Ich gebe dir einen Nachrichtenartikel.\n",
    "    Deine Aufgabe:\n",
    "    1. Identifiziere, welches dieser Unternehmen am stärksten betroffen ist: NVDA, TSLA, ASML, META, AMZN.\n",
    "    2. Analysiere logisch, welchen Einfluss diese Nachricht auf den Aktienkurs hat (Positiv/Negativ/Neutral) und warum.\n",
    "    3. Bestimme ein logisches \"Zeitfenster des Einflusses\" (Time Window).\n",
    "       Beispiel: Eine Quartalszahl wirkt sofort (0 bis 1 Tag), eine Produktankündigung vielleicht länger (0 bis 7 Tage), ein Skandal (0 bis 14 Tage).\n",
    "       Du kannst auch negative Tage angeben (z.B. -1 für \"Gerüchte kurz vor Veröffentlichung\").\n",
    "\n",
    "    Gib deine Antwort NUR als valides JSON zurück in folgendem Format:\n",
    "    {{\n",
    "        \"ticker\": \"TICKER_SYMBOL\",\n",
    "        \"impact_score\": \"float zwischen -1.0 (sehr negativ) und 1.0 (sehr positiv)\",\n",
    "        \"reasoning\": \"Kurze Begründung deiner Einschätzung...\",\n",
    "        \"time_window_start_days\": int (Offset in Tagen zum Publikationsdatum, z.B. 0),\n",
    "        \"time_window_end_days\": int (Offset in Tagen zum Publikationsdatum, z.B. 5)\n",
    "    }}\n",
    "\n",
    "    Artikel Text:\n",
    "    {article_text}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        # Bereinigen des Outputs (falls Markdown ```json enthalten ist)\n",
    "        text_response = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        return json.loads(text_response)\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini Error: {e}\")\n",
    "        return None"
   ],
   "id": "a9d1354d535b3590",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:30:58.543146Z",
     "start_time": "2025-12-07T01:29:39.910365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "# Erstelle Ordner für die Plots, falls nicht vorhanden\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.makedirs(\"plots\")\n",
    "\n",
    "print(\"Starte Analyse der Top 20 Artikel...\\n\")\n",
    "\n",
    "for index, row in df_news.iterrows():\n",
    "    print(f\"--- Verarbeite Artikel {index + 1}/20 ---\")\n",
    "\n",
    "    url = row.get('url') # Stelle sicher, dass die Spalte in deinem CSV 'url' heißt\n",
    "    pub_date_str = row.get('date') # Stelle sicher, dass Spalte 'date' heißt\n",
    "\n",
    "    # Datum parsen (Anpassen je nach deinem CSV Format, z.B. YYYY-MM-DD)\n",
    "    try:\n",
    "        pub_date = pd.to_datetime(pub_date_str)\n",
    "    except:\n",
    "        pub_date = datetime.now() # Fallback\n",
    "\n",
    "    # 1. Artikel lesen\n",
    "    if not url:\n",
    "        print(\"Keine URL vorhanden, überspringe.\")\n",
    "        continue\n",
    "\n",
    "    content = fetch_article_content(url)\n",
    "\n",
    "    if not content or len(content) < 100:\n",
    "        print(\"Konnte Artikelinhalt nicht lesen oder zu kurz.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Gemini Analyse\n",
    "    analysis = analyze_with_gemini(content, pub_date)\n",
    "\n",
    "    if not analysis:\n",
    "        print(\"Keine Analyse von Gemini erhalten.\")\n",
    "        continue\n",
    "\n",
    "    # Daten extrahieren\n",
    "    ticker = analysis['ticker']\n",
    "    start_offset = analysis['time_window_start_days']\n",
    "    end_offset = analysis['time_window_end_days']\n",
    "    reasoning = analysis['reasoning']\n",
    "\n",
    "    # Prüfen ob Ticker in unseren Daten ist\n",
    "    if ticker not in stock_data:\n",
    "        print(f\"Ticker {ticker} nicht in unseren CSV-Dateien (NVDA, TSLA, etc.).\")\n",
    "        continue\n",
    "\n",
    "    # 3. Zeitfenster berechnen (Auf volle Tage gerundet wie gewünscht)\n",
    "    start_date = pub_date + timedelta(days=start_offset)\n",
    "    end_date = pub_date + timedelta(days=end_offset)\n",
    "\n",
    "    # 4. Aktiendaten für das Zeitfenster holen\n",
    "    df_stock = stock_data[ticker]\n",
    "\n",
    "    # Wir filtern den DataFrame auf das Zeitfenster\n",
    "    # Wir fügen etwas Puffer hinzu für den Plot (z.B. 2 Tage davor und danach für Kontext)\n",
    "    plot_start = start_date - timedelta(days=2)\n",
    "    plot_end = end_date + timedelta(days=2)\n",
    "\n",
    "    mask = (df_stock.index >= plot_start) & (df_stock.index <= plot_end)\n",
    "    window_data = df_stock.loc[mask]\n",
    "\n",
    "    if window_data.empty:\n",
    "        print(f\"Keine Aktiendaten für {ticker} im Zeitraum {start_date.date()} bis {end_date.date()} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    # 5. Visualisierung\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Annahme: Spalte für Schließkurs heißt 'Close' oder 'Adj Close'. Nimm die erste gefundene numerische Spalte sonst.\n",
    "    price_col = [c for c in window_data.columns if 'Close' in c]\n",
    "    price_col = price_col[0] if price_col else window_data.columns[0]\n",
    "\n",
    "    plt.plot(window_data.index, window_data[price_col], marker='o', label=f'{ticker} Price')\n",
    "\n",
    "    # Markiere das von Gemini definierte \"Einfluss-Fenster\"\n",
    "    plt.axvspan(start_date, end_date, color='yellow', alpha=0.3, label='Gemini Impact Window')\n",
    "    plt.axvline(pub_date, color='red', linestyle='--', label='News Pub Date')\n",
    "\n",
    "    plt.title(f\"News Impact: {ticker} | {pub_date.date()}\\nReason: {reasoning[:60]}...\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Speichern des Plots\n",
    "    plot_filename = f\"plots/article_{index}_{ticker}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close() # Plot schließen um Speicher zu sparen\n",
    "\n",
    "    # 6. Ergebnisse sammeln\n",
    "    result_entry = {\n",
    "        \"article_id\": index,\n",
    "        \"ticker\": ticker,\n",
    "        \"pub_date\": str(pub_date),\n",
    "        \"gemini_start_window\": str(start_date.date()),\n",
    "        \"gemini_end_window\": str(end_date.date()),\n",
    "        \"reasoning\": reasoning,\n",
    "        \"impact_score\": analysis['impact_score'],\n",
    "        \"plot_file\": plot_filename,\n",
    "        \"original_url\": url\n",
    "    }\n",
    "    results.append(result_entry)\n",
    "    print(f\"Fertig: {ticker}, Fenster: {start_offset} bis {end_offset} Tage. Plot gespeichert.\")\n",
    "\n",
    "    # Kurze Pause um API Limits zu respektieren (bei Free Tier wichtig)\n",
    "    time.sleep(4)\n",
    "\n",
    "print(\"\\nAlle Artikel verarbeitet.\")"
   ],
   "id": "81e2a1433f3094d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Analyse der Top 20 Artikel...\n",
      "\n",
      "--- Verarbeite Artikel 1/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 2/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 3/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 4/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 5/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 6/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 7/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 8/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 9/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 10/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 11/20 ---\n",
      "Konnte Artikelinhalt nicht lesen oder zu kurz.\n",
      "--- Verarbeite Artikel 12/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 13/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 14/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 15/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 16/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 17/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 18/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 19/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "--- Verarbeite Artikel 20/20 ---\n",
      "Gemini Error: 403 Your API key was reported as leaked. Please use another API key.\n",
      "Keine Analyse von Gemini erhalten.\n",
      "\n",
      "Alle Artikel verarbeitet.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:30:59.339227Z",
     "start_time": "2025-12-07T01:30:59.265729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Als JSON speichern\n",
    "with open('final_analysis_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Als CSV speichern für einfache Ansicht\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('final_analysis_results.csv', index=False)\n",
    "\n",
    "print(\"Analyse abgeschlossen. Ergebnisse gespeichert in 'final_analysis_results.csv' und 'final_analysis_results.json'.\")\n",
    "print(f\"Anzahl erstellter Analysen: {len(df_results)}\")\n",
    "df_results.head()"
   ],
   "id": "49270c704675f953",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse abgeschlossen. Ergebnisse gespeichert in 'final_analysis_results.csv' und 'final_analysis_results.json'.\n",
      "Anzahl erstellter Analysen: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:30:59.467515Z",
     "start_time": "2025-12-07T01:30:59.459436Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dd3ccb109cbda0c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ddb7945318c3e61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T01:30:59.910608Z",
     "start_time": "2025-12-07T01:30:59.900428Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cc1ec0f717cc8ab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dbd0200b15759a73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "364db66af9cde8aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "230a5c7730c31b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
